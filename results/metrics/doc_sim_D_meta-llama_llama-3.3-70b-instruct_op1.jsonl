{"meta": {"group": "D", "topic_id": 0, "topic_size": 1893, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "peer review", "stats": {"n_snippets": 10, "mean": 0.3164425492286682, "min": -0.0748692974448204, "max": 0.6420503854751587}}
{"meta": {"group": "D", "topic_id": 1, "topic_size": 24, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "emergency medical care", "stats": {"n_snippets": 10, "mean": 0.3129549026489258, "min": 0.12889286875724792, "max": 0.4645504355430603}}
{"meta": {"group": "D", "topic_id": 2, "topic_size": 20, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "tech support issues", "stats": {"n_snippets": 10, "mean": 0.30279651284217834, "min": 0.07966932654380798, "max": 0.5267723798751831}}
{"meta": {"group": "D", "topic_id": 3, "topic_size": 15, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "deducer install issues", "stats": {"n_snippets": 10, "mean": 0.4251008629798889, "min": 0.19232334196567535, "max": 0.658264696598053}}
{"meta": {"group": "D", "topic_id": 4, "topic_size": 12, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "download slides", "stats": {"n_snippets": 10, "mean": 0.6406580209732056, "min": 0.4333355724811554, "max": 0.9332292079925537}}
{"meta": {"group": "D", "topic_id": 5, "topic_size": 11, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "histogram basics", "stats": {"n_snippets": 10, "mean": 0.564254641532898, "min": 0.35335010290145874, "max": 0.6529924273490906}}
{"meta": {"group": "D", "topic_id": 6, "topic_size": 10, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "paediatric emergency care", "stats": {"n_snippets": 10, "mean": 0.25015130639076233, "min": 0.07285536825656891, "max": 0.3864273726940155}}
{"meta": {"group": "D", "topic_id": 7, "topic_size": 10, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "spinal cord injuries", "stats": {"n_snippets": 10, "mean": 0.6053305864334106, "min": 0.3465242087841034, "max": 0.7532583475112915}}
{"meta": {"group": "D", "topic_id": 8, "topic_size": 9, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "person years", "stats": {"n_snippets": 9, "mean": 0.5085988640785217, "min": 0.329098105430603, "max": 0.7820295095443726}}
{"meta": {"group": "D", "topic_id": 9, "topic_size": 9, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "accomplishment statement", "stats": {"n_snippets": 9, "mean": 0.5850631594657898, "min": 0.32093095779418945, "max": 0.819157600402832}}
{"meta": {"group": "D", "topic_id": 10, "topic_size": 9, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "toluene exposure risk", "stats": {"n_snippets": 9, "mean": 0.26947879791259766, "min": 0.19001725316047668, "max": 0.4243321418762207}}
{"meta": {"group": "D", "topic_id": 11, "topic_size": 8, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "easy compose topic", "stats": {"n_snippets": 8, "mean": 0.49813923239707947, "min": 0.232069730758667, "max": 0.5643283128738403}}
{"meta": {"group": "D", "topic_id": 12, "topic_size": 7, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "course support", "stats": {"n_snippets": 7, "mean": 0.1970950812101364, "min": 0.08875054121017456, "max": 0.3579447865486145}}
{"meta": {"group": "D", "topic_id": 13, "topic_size": 7, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "normal approximation", "stats": {"n_snippets": 7, "mean": 0.49709171056747437, "min": 0.401789128780365, "max": 0.576065182685852}}
{"meta": {"group": "D", "topic_id": 14, "topic_size": 7, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "ordinal data", "stats": {"n_snippets": 7, "mean": 0.498280793428421, "min": 0.313204824924469, "max": 0.642818808555603}}
{"meta": {"group": "D", "topic_id": 15, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "multiple births", "stats": {"n_snippets": 6, "mean": 0.4206053912639618, "min": 0.0753391832113266, "max": 0.6190690398216248}}
{"meta": {"group": "D", "topic_id": 16, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "slope calculation", "stats": {"n_snippets": 6, "mean": 0.542107880115509, "min": 0.3920656442642212, "max": 0.7603926658630371}}
{"meta": {"group": "D", "topic_id": 17, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "getting help", "stats": {"n_snippets": 6, "mean": 0.36605212092399597, "min": 0.15893760323524475, "max": 0.5701563954353333}}
{"meta": {"group": "D", "topic_id": 18, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "donor blood prevalence", "stats": {"n_snippets": 6, "mean": 0.4876220226287842, "min": 0.3674626350402832, "max": 0.6266674995422363}}
{"meta": {"group": "D", "topic_id": 19, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "language barrier", "stats": {"n_snippets": 6, "mean": 0.23260651528835297, "min": 0.12000303715467453, "max": 0.3481849133968353}}
{"meta": {"group": "D", "topic_id": 20, "topic_size": 6, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "electron trajectories", "stats": {"n_snippets": 6, "mean": 0.5349597334861755, "min": 0.26968491077423096, "max": 0.6976271867752075}}
{"meta": {"group": "D", "topic_id": 21, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "rolling dice probability", "stats": {"n_snippets": 5, "mean": 0.6442039608955383, "min": 0.5161360502243042, "max": 0.7307382225990295}}
{"meta": {"group": "D", "topic_id": 22, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "demo edit issues", "stats": {"n_snippets": 5, "mean": 0.5335544943809509, "min": 0.27400505542755127, "max": 0.6633133888244629}}
{"meta": {"group": "D", "topic_id": 23, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "email registration", "stats": {"n_snippets": 5, "mean": 0.4622626304626465, "min": 0.38064271211624146, "max": 0.5312228202819824}}
{"meta": {"group": "D", "topic_id": 24, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "beta variable comparison", "stats": {"n_snippets": 5, "mean": 0.3625599443912506, "min": 0.162138432264328, "max": 0.5555437803268433}}
{"meta": {"group": "D", "topic_id": 25, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "data interpretation", "stats": {"n_snippets": 5, "mean": 0.3054778575897217, "min": 0.10475297272205353, "max": 0.49742594361305237}}
{"meta": {"group": "D", "topic_id": 26, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "pvalue adjustment", "stats": {"n_snippets": 5, "mean": 0.5003575086593628, "min": 0.2961815297603607, "max": 0.8173567056655884}}
{"meta": {"group": "D", "topic_id": 27, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "halt study", "stats": {"n_snippets": 5, "mean": 0.6956287622451782, "min": 0.5682175159454346, "max": 0.8176994323730469}}
{"meta": {"group": "D", "topic_id": 28, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "alzheimers prevention", "stats": {"n_snippets": 5, "mean": 0.62061607837677, "min": 0.28545278310775757, "max": 0.785010814666748}}
{"meta": {"group": "D", "topic_id": 29, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "quiz value help", "stats": {"n_snippets": 5, "mean": 0.3806399703025818, "min": 0.2614026665687561, "max": 0.46370929479599}}
{"meta": {"group": "D", "topic_id": 30, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "null hypothesis testing", "stats": {"n_snippets": 5, "mean": 0.3123205304145813, "min": 0.13856276869773865, "max": 0.5740297436714172}}
{"meta": {"group": "D", "topic_id": 31, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "cardiorespiratory fitness", "stats": {"n_snippets": 5, "mean": 0.6825259923934937, "min": 0.5604640245437622, "max": 0.7999398708343506}}
{"meta": {"group": "D", "topic_id": 32, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "binomial probability", "stats": {"n_snippets": 5, "mean": 0.43258658051490784, "min": 0.33078092336654663, "max": 0.528336763381958}}
{"meta": {"group": "D", "topic_id": 33, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "frequency data", "stats": {"n_snippets": 5, "mean": 0.5576525926589966, "min": 0.4099538326263428, "max": 0.6996264457702637}}
{"meta": {"group": "D", "topic_id": 34, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "survival rate", "stats": {"n_snippets": 5, "mean": 0.4710080027580261, "min": 0.30377259850502014, "max": 0.5826408863067627}}
{"meta": {"group": "D", "topic_id": 35, "topic_size": 5, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "finding old posts", "stats": {"n_snippets": 5, "mean": 0.44118133187294006, "min": 0.27203184366226196, "max": 0.6748850345611572}}
{"meta": {"group": "D", "topic_id": 36, "topic_size": 4, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "vaccine simulation", "stats": {"n_snippets": 4, "mean": 0.6318832039833069, "min": 0.3417799174785614, "max": 0.7827852964401245}}
{"meta": {"group": "D", "topic_id": 37, "topic_size": 4, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "mock data download", "stats": {"n_snippets": 4, "mean": 0.5848710536956787, "min": 0.41411641240119934, "max": 0.7750372886657715}}
{"meta": {"group": "D", "topic_id": 38, "topic_size": 4, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "module repetition issue", "stats": {"n_snippets": 4, "mean": 0.6179367303848267, "min": 0.42083367705345154, "max": 0.8487237691879272}}
{"meta": {"group": "D", "topic_id": 39, "topic_size": 4, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "fishers exact test", "stats": {"n_snippets": 4, "mean": 0.48996344208717346, "min": 0.2513519525527954, "max": 0.5949879884719849}}
{"meta": {"group": "D", "topic_id": 40, "topic_size": 4, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "coin flip bias", "stats": {"n_snippets": 4, "mean": 0.4900338351726532, "min": 0.37540411949157715, "max": 0.6323571801185608}}
{"meta": {"group": "D", "topic_id": 41, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "unit download issues", "stats": {"n_snippets": 3, "mean": 0.525764524936676, "min": 0.30227336287498474, "max": 0.6468549370765686}}
{"meta": {"group": "D", "topic_id": 42, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "verb tense change", "stats": {"n_snippets": 3, "mean": 0.2825310528278351, "min": 0.05787203833460808, "max": 0.4563212990760803}}
{"meta": {"group": "D", "topic_id": 43, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "submit button issue", "stats": {"n_snippets": 3, "mean": 0.3537009656429291, "min": 0.22555872797966003, "max": 0.498771607875824}}
{"meta": {"group": "D", "topic_id": 44, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "common sense", "stats": {"n_snippets": 3, "mean": 0.6037551164627075, "min": 0.550006628036499, "max": 0.6829513311386108}}
{"meta": {"group": "D", "topic_id": 45, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "distance calculation", "stats": {"n_snippets": 3, "mean": 0.40051186084747314, "min": 0.3522840738296509, "max": 0.4905279576778412}}
{"meta": {"group": "D", "topic_id": 46, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "five letter words", "stats": {"n_snippets": 3, "mean": 0.5132425427436829, "min": 0.34901928901672363, "max": 0.7592923045158386}}
{"meta": {"group": "D", "topic_id": 47, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "need help", "stats": {"n_snippets": 3, "mean": 0.7619953155517578, "min": 0.6736159324645996, "max": 0.8061850070953369}}
{"meta": {"group": "D", "topic_id": 48, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "amputee sprinters", "stats": {"n_snippets": 3, "mean": 0.5468551516532898, "min": 0.3381621837615967, "max": 0.699970543384552}}
{"meta": {"group": "D", "topic_id": 49, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "raised nevi study", "stats": {"n_snippets": 3, "mean": 0.4594762623310089, "min": 0.3681159019470215, "max": 0.5890432000160217}}
{"meta": {"group": "D", "topic_id": 50, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "dsst score increase", "stats": {"n_snippets": 3, "mean": 0.503959596157074, "min": 0.40505123138427734, "max": 0.5708838701248169}}
{"meta": {"group": "D", "topic_id": 51, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "signrank test", "stats": {"n_snippets": 3, "mean": 0.4322606027126312, "min": 0.33206599950790405, "max": 0.6308538913726807}}
{"meta": {"group": "D", "topic_id": 52, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "data analysis issue", "stats": {"n_snippets": 3, "mean": 0.3640371263027191, "min": 0.3083314895629883, "max": 0.4212331473827362}}
{"meta": {"group": "D", "topic_id": 53, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "cheating concerns", "stats": {"n_snippets": 3, "mean": 0.2923593819141388, "min": 0.07879003137350082, "max": 0.6509045958518982}}
{"meta": {"group": "D", "topic_id": 54, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "card combinations", "stats": {"n_snippets": 3, "mean": 0.552254855632782, "min": 0.5127004384994507, "max": 0.6201521158218384}}
{"meta": {"group": "D", "topic_id": 55, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "ratios calculation help", "stats": {"n_snippets": 3, "mean": 0.40067294239997864, "min": 0.2323756366968155, "max": 0.589063823223114}}
{"meta": {"group": "D", "topic_id": 56, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "act occurs", "stats": {"n_snippets": 3, "mean": 0.2153383046388626, "min": 0.09387440234422684, "max": 0.33806759119033813}}
{"meta": {"group": "D", "topic_id": 57, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "exact test", "stats": {"n_snippets": 3, "mean": 0.37318792939186096, "min": 0.33881330490112305, "max": 0.4053022265434265}}
{"meta": {"group": "D", "topic_id": 58, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "browser issues", "stats": {"n_snippets": 3, "mean": 0.39998936653137207, "min": 0.3239838182926178, "max": 0.4861627221107483}}
{"meta": {"group": "D", "topic_id": 59, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "missing data", "stats": {"n_snippets": 3, "mean": 0.41896772384643555, "min": 0.27408915758132935, "max": 0.5939955115318298}}
{"meta": {"group": "D", "topic_id": 60, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "decimal places", "stats": {"n_snippets": 3, "mean": 0.5255928039550781, "min": 0.5255928039550781, "max": 0.5255928039550781}}
{"meta": {"group": "D", "topic_id": 61, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "answer help", "stats": {"n_snippets": 3, "mean": 0.6036798357963562, "min": 0.512559175491333, "max": 0.6709070205688477}}
{"meta": {"group": "D", "topic_id": 62, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "paper clarity issues", "stats": {"n_snippets": 3, "mean": 0.5297157764434814, "min": 0.5297157764434814, "max": 0.5297157764434814}}
{"meta": {"group": "D", "topic_id": 63, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "number format", "stats": {"n_snippets": 3, "mean": 0.3436901569366455, "min": 0.21152108907699585, "max": 0.5729555487632751}}
{"meta": {"group": "D", "topic_id": 64, "topic_size": 3, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "prevalence calculation", "stats": {"n_snippets": 3, "mean": 0.4486069977283478, "min": 0.3519144058227539, "max": 0.5079019069671631}}
{"meta": {"group": "D", "topic_id": 65, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "predictive power", "stats": {"n_snippets": 2, "mean": 0.13270865380764008, "min": -0.013756360858678818, "max": 0.27917367219924927}}
{"meta": {"group": "D", "topic_id": 66, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "alzheimers disease", "stats": {"n_snippets": 2, "mean": 0.51701819896698, "min": 0.4453905522823334, "max": 0.5886458158493042}}
{"meta": {"group": "D", "topic_id": 67, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "wait for fix", "stats": {"n_snippets": 2, "mean": 0.301779568195343, "min": 0.23589946329593658, "max": 0.36765968799591064}}
{"meta": {"group": "D", "topic_id": 68, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "standard normal chart", "stats": {"n_snippets": 2, "mean": 0.6341344118118286, "min": 0.6039055585861206, "max": 0.6643632650375366}}
{"meta": {"group": "D", "topic_id": 69, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "journal article review", "stats": {"n_snippets": 2, "mean": 0.41019436717033386, "min": 0.3264821171760559, "max": 0.4939066171646118}}
{"meta": {"group": "D", "topic_id": 70, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "binomial distribution", "stats": {"n_snippets": 2, "mean": 0.5328116416931152, "min": 0.35853278636932373, "max": 0.707090437412262}}
{"meta": {"group": "D", "topic_id": 71, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "climate health management", "stats": {"n_snippets": 2, "mean": 0.40105730295181274, "min": 0.3925796449184418, "max": 0.40953493118286133}}
{"meta": {"group": "D", "topic_id": 72, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "childhood conduct problems", "stats": {"n_snippets": 2, "mean": 0.4231038987636566, "min": 0.17086444795131683, "max": 0.6753433346748352}}
{"meta": {"group": "D", "topic_id": 73, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "zero grade complaints", "stats": {"n_snippets": 2, "mean": 0.5040225982666016, "min": 0.46610453724861145, "max": 0.5419406890869141}}
{"meta": {"group": "D", "topic_id": 74, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "video solution error", "stats": {"n_snippets": 2, "mean": 0.33653727173805237, "min": 0.10066896677017212, "max": 0.5724055767059326}}
{"meta": {"group": "D", "topic_id": 75, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "spinal cord injuries", "stats": {"n_snippets": 2, "mean": 0.29085415601730347, "min": 0.12526869773864746, "max": 0.4564396142959595}}
{"meta": {"group": "D", "topic_id": 76, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "statement receipt", "stats": {"n_snippets": 2, "mean": 0.5238232016563416, "min": 0.4519292712211609, "max": 0.5957171320915222}}
{"meta": {"group": "D", "topic_id": 77, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "outlier detection", "stats": {"n_snippets": 2, "mean": 0.4177883267402649, "min": 0.3819947838783264, "max": 0.453581839799881}}
{"meta": {"group": "D", "topic_id": 78, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "forecasting errors", "stats": {"n_snippets": 2, "mean": 0.328404039144516, "min": 0.25942736864089966, "max": 0.3973807096481323}}
{"meta": {"group": "D", "topic_id": 79, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "module download issue", "stats": {"n_snippets": 2, "mean": 0.7545674443244934, "min": 0.7059767246246338, "max": 0.803158164024353}}
{"meta": {"group": "D", "topic_id": 80, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "peer editing issues", "stats": {"n_snippets": 2, "mean": 0.33748286962509155, "min": 0.2957286834716797, "max": 0.3792370557785034}}
{"meta": {"group": "D", "topic_id": 81, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "update reference books", "stats": {"n_snippets": 2, "mean": 0.6132762432098389, "min": 0.508177638053894, "max": 0.7183749079704285}}
{"meta": {"group": "D", "topic_id": 82, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "rescuetime uninstall help", "stats": {"n_snippets": 2, "mean": 0.5187591314315796, "min": 0.26397138833999634, "max": 0.7735468149185181}}
{"meta": {"group": "D", "topic_id": 83, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "independent events", "stats": {"n_snippets": 2, "mean": 0.4038606286048889, "min": 0.36046102643013, "max": 0.4472602605819702}}
{"meta": {"group": "D", "topic_id": 84, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "odds ratios", "stats": {"n_snippets": 2, "mean": 0.3336489200592041, "min": 0.181146502494812, "max": 0.4861513078212738}}
{"meta": {"group": "D", "topic_id": 85, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "lecture hints", "stats": {"n_snippets": 2, "mean": 0.5093923211097717, "min": 0.3796962797641754, "max": 0.6390883922576904}}
{"meta": {"group": "D", "topic_id": 86, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "submit assignment help", "stats": {"n_snippets": 2, "mean": 0.3021802306175232, "min": 0.23039008677005768, "max": 0.3739703893661499}}
{"meta": {"group": "D", "topic_id": 87, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "statistical inference", "stats": {"n_snippets": 2, "mean": 0.29032522439956665, "min": 0.05381542816758156, "max": 0.526835024356842}}
{"meta": {"group": "D", "topic_id": 88, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "linear regression", "stats": {"n_snippets": 2, "mean": 0.3692918121814728, "min": 0.32264524698257446, "max": 0.4159383773803711}}
{"meta": {"group": "D", "topic_id": 89, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "meeting deadlines", "stats": {"n_snippets": 2, "mean": 0.3382219672203064, "min": 0.32711514830589294, "max": 0.34932875633239746}}
{"meta": {"group": "D", "topic_id": 90, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "rsquare interpretation", "stats": {"n_snippets": 2, "mean": 0.38830137252807617, "min": 0.13062897324562073, "max": 0.645973801612854}}
{"meta": {"group": "D", "topic_id": 91, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "course access help", "stats": {"n_snippets": 2, "mean": 0.40069884061813354, "min": 0.3999253511428833, "max": 0.4014723300933838}}
{"meta": {"group": "D", "topic_id": 92, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "blood sugar", "stats": {"n_snippets": 2, "mean": 0.2059425413608551, "min": 0.13624738156795502, "max": 0.275637686252594}}
{"meta": {"group": "D", "topic_id": 93, "topic_size": 2, "model_tag": "meta-llama_llama-3.3-70b-instruct", "prompt_option": 1}, "label": "discordant pairs", "stats": {"n_snippets": 2, "mean": 0.27478066086769104, "min": 0.18187598884105682, "max": 0.36768531799316406}}
